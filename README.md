# LLM Router

Service de routage intelligent pour les requÃªtes LLM. Choix automatique du meilleur modÃ¨le selon le type de tÃ¢che (code, reasoning, conversation, tools).

## Ã‰tat du projet

| Phase | Description | Status |
|-------|-------------|--------|
| Phase 1 | Forward-only vers OpenRouter | âœ… ComplÃ¨te |
| Phase 2 | Routing par keywords + monitoring | âœ… ComplÃ¨te |
| Phase 3 | Routing LLM-based (Ollama/API) | ðŸ”„ En cours |

## DÃ©marrage rapide

```bash
cd service
pip install -r requirements.txt
cp .env.example .env
# Ã‰diter .env avec votre OPENROUTER_API_KEY
uvicorn main:app --host 0.0.0.0 --port 3456
```

## Endpoints

| Endpoint | Description |
|----------|-------------|
| `POST /v1/chat/completions` | OpenAI-compatible chat completions |
| `POST /chat/completions` | Alias (compatibilitÃ© OpenClaw) |
| `GET /health` | Health check |
| `GET /metrics` | MÃ©triques (requests, latence, coÃ»t, circuit breaker) |
| `GET /config` | Configuration actuelle |
| `POST /config/category` | Ajouter/modifier une catÃ©gorie |
| `POST /config/model-mapping` | Modifier les modÃ¨les d'une catÃ©gorie |
| `DELETE /config/category/{name}` | Supprimer une catÃ©gorie personnalisÃ©e |
| `POST /circuit-breaker/reset/{model}` | RÃ©initialiser le circuit breaker |

## Routing actuel

DÃ©tection par keywords + support tools:

| CatÃ©gorie | DÃ©tection | ModÃ¨les (fallback chain) |
|-----------|-----------|--------------------------|
| **tools** | `request.tools` prÃ©sent | aurora-alpha â†’ kimi-k2.5 â†’ glm-5 |
| **code** | keywords: python, function, debug... | glm-5 â†’ aurora-alpha â†’ gpt-4o-mini |
| **reasoning** | keywords: why, how, explain... | aurora-alpha â†’ glm-5 â†’ kimi-k2.5 |
| **conversation** | messages courts, hello, thanks... | glm-5 â†’ gpt-4o-mini â†’ aurora-alpha |

## Configuration OpenClaw

Ajouter dans `~/.openclaw/openclaw.json`:

```json
{
  "models": {
    "providers": {
      "router": {
        "baseUrl": "http://localhost:3456",
        "apiKey": "local-router",
        "api": "openai-completions",
        "models": [{ "id": "router", "name": "LLM Router" }]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "router/router"
      }
    }
  }
}
```

## Architecture

```
llm-router/
â”œâ”€â”€ README.md                # Ce fichier
â”œâ”€â”€ overview.md              # Vue d'ensemble
â”œâ”€â”€ steps.md                 # Ã‰tapes de dÃ©veloppement
â”œâ”€â”€ routing-engine.md        # Moteur de routing
â”œâ”€â”€ model-calls.md           # Appels aux modÃ¨les
â”œâ”€â”€ fallback-guardrails.md   # Fallbacks et sÃ©curitÃ©s
â”œâ”€â”€ monitoring.md            # MÃ©triques
â”œâ”€â”€ integration.md           # IntÃ©gration OpenClaw
â””â”€â”€ service/
    â”œâ”€â”€ main.py              # Service FastAPI
    â”œâ”€â”€ requirements.txt     # DÃ©pendances Python
    â””â”€â”€ .env.example         # Config exemple
```

## Monitoring

```bash
curl http://localhost:3456/metrics
```

Retourne:
- Nombre de requÃªtes (total/succÃ¨s/Ã©chec)
- Latence moyenne
- Distribution par modÃ¨le et catÃ©gorie
- 10 derniÃ¨res requÃªtes
